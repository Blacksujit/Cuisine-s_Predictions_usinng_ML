{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression , LogisticRegression\n",
    "from mlxtend.plotting import decision_regions , plot_decision_regions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"Dataset .csv\"\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant ID</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>City</th>\n",
       "      <th>Address</th>\n",
       "      <th>Locality</th>\n",
       "      <th>Locality Verbose</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Average Cost for two</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Has Table booking</th>\n",
       "      <th>Has Online delivery</th>\n",
       "      <th>Is delivering now</th>\n",
       "      <th>Switch to order menu</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Aggregate rating</th>\n",
       "      <th>Rating color</th>\n",
       "      <th>Rating text</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6317637</td>\n",
       "      <td>Le Petit Souffle</td>\n",
       "      <td>162</td>\n",
       "      <td>Makati City</td>\n",
       "      <td>Third Floor, Century City Mall, Kalayaan Avenu...</td>\n",
       "      <td>Century City Mall, Poblacion, Makati City</td>\n",
       "      <td>Century City Mall, Poblacion, Makati City, Mak...</td>\n",
       "      <td>121.027535</td>\n",
       "      <td>14.565443</td>\n",
       "      <td>French, Japanese, Desserts</td>\n",
       "      <td>1100</td>\n",
       "      <td>Botswana Pula(P)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Dark Green</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6304287</td>\n",
       "      <td>Izakaya Kikufuji</td>\n",
       "      <td>162</td>\n",
       "      <td>Makati City</td>\n",
       "      <td>Little Tokyo, 2277 Chino Roces Avenue, Legaspi...</td>\n",
       "      <td>Little Tokyo, Legaspi Village, Makati City</td>\n",
       "      <td>Little Tokyo, Legaspi Village, Makati City, Ma...</td>\n",
       "      <td>121.014101</td>\n",
       "      <td>14.553708</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>1200</td>\n",
       "      <td>Botswana Pula(P)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Dark Green</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6300002</td>\n",
       "      <td>Heat - Edsa Shangri-La</td>\n",
       "      <td>162</td>\n",
       "      <td>Mandaluyong City</td>\n",
       "      <td>Edsa Shangri-La, 1 Garden Way, Ortigas, Mandal...</td>\n",
       "      <td>Edsa Shangri-La, Ortigas, Mandaluyong City</td>\n",
       "      <td>Edsa Shangri-La, Ortigas, Mandaluyong City, Ma...</td>\n",
       "      <td>121.056831</td>\n",
       "      <td>14.581404</td>\n",
       "      <td>Seafood, Asian, Filipino, Indian</td>\n",
       "      <td>4000</td>\n",
       "      <td>Botswana Pula(P)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Green</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Restaurant ID         Restaurant Name  ...  Rating text Votes\n",
       "0        6317637        Le Petit Souffle  ...    Excellent   314\n",
       "1        6304287        Izakaya Kikufuji  ...    Excellent   591\n",
       "2        6300002  Heat - Edsa Shangri-La  ...    Very Good   270\n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>City</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Price range</th>\n",
       "      <th>Aggregate rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le Petit Souffle</td>\n",
       "      <td>73</td>\n",
       "      <td>920</td>\n",
       "      <td>3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Izakaya Kikufuji</td>\n",
       "      <td>73</td>\n",
       "      <td>1111</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heat - Edsa Shangri-La</td>\n",
       "      <td>75</td>\n",
       "      <td>1671</td>\n",
       "      <td>4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ooma</td>\n",
       "      <td>75</td>\n",
       "      <td>1126</td>\n",
       "      <td>4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sambo Kojin</td>\n",
       "      <td>75</td>\n",
       "      <td>1122</td>\n",
       "      <td>4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Restaurant Name  City  Cuisines  Price range  Aggregate rating  Votes\n",
       "0        Le Petit Souffle    73       920            3               4.8    314\n",
       "1        Izakaya Kikufuji    73      1111            3               4.5    591\n",
       "2  Heat - Edsa Shangri-La    75      1671            4               4.4    270\n",
       "3                    Ooma    75      1126            4               4.9    365\n",
       "4             Sambo Kojin    75      1122            4               4.8    229"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Selecting relevant columns\n",
    "relevant_columns = ['Restaurant Name', 'City', 'Cuisines', 'Price range', 'Aggregate rating', 'Votes']\n",
    "data = data[relevant_columns]\n",
    "\n",
    "# Handling missing values by filling with a placeholder\n",
    "data.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['City'] = label_encoder.fit_transform(data['City'])\n",
    "\n",
    "# Encoding the target variable 'Cuisines'\n",
    "data['Cuisines'] = label_encoder.fit_transform(data['Cuisines'])\n",
    "\n",
    "# Display the processed data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7640, 5), (1911, 5), (7640,), (1911,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining features and target variable\n",
    "X = data[['Restaurant Name', 'City', 'Price range', 'Aggregate rating', 'Votes']]\n",
    "y = data['Cuisines']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['City', 'Price range', 'Aggregate rating', 'Votes']]\n",
    "y = data['Cuisines']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Initializing the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.07953950811093669\n",
      "Precision: 0.04263841195473863\n",
      "Recall: 0.07953950811093669\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         0\n",
      "          43       0.00      0.00      0.00         0\n",
      "          44       0.00      0.00      0.00         0\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          47       0.00      0.00      0.00         0\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         0\n",
      "          54       0.00      0.00      0.00         3\n",
      "          55       0.00      0.00      0.00         3\n",
      "          58       0.00      0.00      0.00         8\n",
      "          59       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         1\n",
      "          66       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         0\n",
      "          74       0.00      0.00      0.00         0\n",
      "          78       0.50      1.00      0.67         1\n",
      "          80       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         1\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       0.00      0.00      0.00         1\n",
      "          85       0.00      0.00      0.00         0\n",
      "          87       0.00      0.00      0.00         0\n",
      "          89       0.00      0.00      0.00         0\n",
      "          97       0.00      0.00      0.00         0\n",
      "          98       0.00      0.00      0.00         0\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       0.00      0.00      0.00         1\n",
      "         102       0.00      0.00      0.00         2\n",
      "         104       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         1\n",
      "         109       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.00      0.00      0.00         0\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         5\n",
      "         120       0.00      0.00      0.00         1\n",
      "         121       0.00      0.00      0.00         2\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.00      0.00      0.00         1\n",
      "         128       0.00      0.00      0.00         0\n",
      "         136       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         0\n",
      "         143       0.00      0.00      0.00         0\n",
      "         148       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         0\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       0.00      0.00      0.00         1\n",
      "         153       0.00      0.00      0.00         0\n",
      "         154       0.00      0.00      0.00         0\n",
      "         156       0.00      0.00      0.00         0\n",
      "         157       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         0\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       0.00      0.00      0.00         0\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         0\n",
      "         176       0.00      0.00      0.00         0\n",
      "         177       0.03      0.03      0.03        36\n",
      "         178       0.00      0.00      0.00         0\n",
      "         180       0.00      0.00      0.00         0\n",
      "         181       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         2\n",
      "         186       0.00      0.00      0.00        32\n",
      "         188       0.00      0.00      0.00         0\n",
      "         190       0.00      0.00      0.00         0\n",
      "         191       0.00      0.00      0.00         9\n",
      "         192       0.00      0.00      0.00         0\n",
      "         195       0.00      0.00      0.00         0\n",
      "         196       0.00      0.00      0.00         1\n",
      "         197       0.00      0.00      0.00         0\n",
      "         199       0.00      0.00      0.00         0\n",
      "         200       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00        21\n",
      "         202       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         1\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         3\n",
      "         218       0.00      0.00      0.00         2\n",
      "         223       0.00      0.00      0.00         1\n",
      "         226       0.00      0.00      0.00         1\n",
      "         228       0.00      0.00      0.00         0\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00        12\n",
      "         233       0.00      0.00      0.00         3\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         1\n",
      "         237       0.00      0.00      0.00         1\n",
      "         239       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         8\n",
      "         246       0.00      0.00      0.00         2\n",
      "         247       0.00      0.00      0.00         0\n",
      "         249       0.00      0.00      0.00         3\n",
      "         251       0.00      0.00      0.00         2\n",
      "         252       0.00      0.00      0.00         1\n",
      "         254       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         1\n",
      "         258       1.00      1.00      1.00         1\n",
      "         259       0.00      0.00      0.00         1\n",
      "         260       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         0\n",
      "         264       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         277       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         1\n",
      "         279       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         0\n",
      "         286       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         1\n",
      "         290       0.00      0.00      0.00         0\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         0\n",
      "         297       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         305       0.00      0.00      0.00         0\n",
      "         307       0.00      0.00      0.00         1\n",
      "         308       0.00      0.00      0.00         1\n",
      "         310       0.00      0.00      0.00        10\n",
      "         311       0.00      0.00      0.00         1\n",
      "         314       0.00      0.00      0.00         1\n",
      "         320       0.00      0.00      0.00         1\n",
      "         322       0.00      0.00      0.00         1\n",
      "         324       0.00      0.00      0.00         0\n",
      "         327       0.00      0.00      0.00         0\n",
      "         329       0.00      0.00      0.00         2\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.02      0.01      0.02        70\n",
      "         335       0.00      0.00      0.00         0\n",
      "         343       0.00      0.00      0.00         0\n",
      "         344       0.00      0.00      0.00         3\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       0.00      0.00      0.00         3\n",
      "         348       0.00      0.00      0.00         1\n",
      "         349       0.00      0.00      0.00         0\n",
      "         351       0.00      0.00      0.00         0\n",
      "         352       0.00      0.00      0.00         1\n",
      "         354       0.00      0.00      0.00         0\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       0.00      0.00      0.00         0\n",
      "         358       0.00      0.00      0.00         1\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         2\n",
      "         362       0.00      0.00      0.00         0\n",
      "         364       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         373       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         1\n",
      "         379       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         0\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         1\n",
      "         388       0.00      0.00      0.00         1\n",
      "         392       0.00      0.00      0.00         0\n",
      "         394       0.00      0.00      0.00         0\n",
      "         395       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         1\n",
      "         398       0.00      0.00      0.00         1\n",
      "         399       0.00      0.00      0.00         1\n",
      "         400       0.00      0.00      0.00         1\n",
      "         402       0.00      0.00      0.00         0\n",
      "         404       0.00      0.00      0.00         0\n",
      "         407       0.00      0.00      0.00         1\n",
      "         410       0.00      0.00      0.00         1\n",
      "         412       0.00      0.00      0.00         2\n",
      "         416       0.00      0.00      0.00         1\n",
      "         419       0.00      0.00      0.00         1\n",
      "         420       0.00      0.00      0.00         1\n",
      "         422       0.00      0.00      0.00         1\n",
      "         426       0.00      0.00      0.00         1\n",
      "         427       0.00      0.00      0.00         0\n",
      "         431       0.00      0.00      0.00         0\n",
      "         433       0.00      0.00      0.00         2\n",
      "         435       0.00      0.00      0.00         0\n",
      "         437       0.00      0.00      0.00         0\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         443       0.00      0.00      0.00         0\n",
      "         444       0.00      0.00      0.00         0\n",
      "         445       0.00      0.00      0.00         1\n",
      "         447       0.00      0.00      0.00         1\n",
      "         448       0.00      0.00      0.00         0\n",
      "         451       0.00      0.00      0.00         1\n",
      "         453       0.00      0.00      0.00         0\n",
      "         455       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         0\n",
      "         468       0.00      0.00      0.00         0\n",
      "         469       0.00      0.00      0.00         0\n",
      "         470       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         484       0.00      0.00      0.00         0\n",
      "         487       0.14      0.33      0.20         3\n",
      "         488       0.00      0.00      0.00         0\n",
      "         493       0.00      0.00      0.00         0\n",
      "         495       0.00      0.00      0.00         0\n",
      "         497       0.02      0.02      0.02        61\n",
      "         498       0.00      0.00      0.00         1\n",
      "         500       0.00      0.00      0.00         1\n",
      "         501       0.00      0.00      0.00         0\n",
      "         502       0.00      0.00      0.00         1\n",
      "         503       0.00      0.00      0.00         0\n",
      "         504       0.00      0.00      0.00         1\n",
      "         505       0.00      0.00      0.00         1\n",
      "         509       0.00      0.00      0.00         1\n",
      "         512       0.00      0.00      0.00         2\n",
      "         516       0.00      0.00      0.00         0\n",
      "         517       0.00      0.00      0.00         0\n",
      "         518       0.06      0.05      0.06        19\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       0.00      0.00      0.00         0\n",
      "         530       0.00      0.00      0.00         0\n",
      "         531       0.00      0.00      0.00         0\n",
      "         533       0.00      0.00      0.00         1\n",
      "         536       0.00      0.00      0.00         1\n",
      "         537       0.00      0.00      0.00         1\n",
      "         545       0.00      0.00      0.00         2\n",
      "         546       0.00      0.00      0.00         4\n",
      "         547       0.00      0.00      0.00         1\n",
      "         549       0.00      0.00      0.00        22\n",
      "         550       0.00      0.00      0.00         0\n",
      "         551       0.00      0.00      0.00         1\n",
      "         552       0.00      0.00      0.00         1\n",
      "         554       0.00      0.00      0.00         1\n",
      "         555       0.00      0.00      0.00         2\n",
      "         556       0.00      0.00      0.00         1\n",
      "         561       0.00      0.00      0.00         1\n",
      "         563       0.00      0.00      0.00         1\n",
      "         564       0.00      0.00      0.00         2\n",
      "         565       0.00      0.00      0.00         1\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       0.00      0.00      0.00         2\n",
      "         575       0.00      0.00      0.00         0\n",
      "         577       0.00      0.00      0.00         0\n",
      "         578       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         1\n",
      "         582       0.00      0.00      0.00         2\n",
      "         583       0.00      0.00      0.00         1\n",
      "         584       0.00      0.00      0.00         0\n",
      "         586       0.00      0.00      0.00         0\n",
      "         588       0.00      0.00      0.00        11\n",
      "         589       0.00      0.00      0.00         3\n",
      "         591       0.00      0.00      0.00         2\n",
      "         593       0.00      0.00      0.00         1\n",
      "         594       0.00      0.00      0.00         0\n",
      "         595       0.00      0.00      0.00         1\n",
      "         600       0.00      0.00      0.00         0\n",
      "         601       0.00      0.00      0.00         1\n",
      "         602       0.00      0.00      0.00         1\n",
      "         604       0.00      0.00      0.00         2\n",
      "         609       0.00      0.00      0.00         0\n",
      "         610       0.00      0.00      0.00         1\n",
      "         611       0.00      0.00      0.00         1\n",
      "         613       0.00      0.00      0.00         1\n",
      "         614       0.00      0.00      0.00         1\n",
      "         615       0.00      0.00      0.00         1\n",
      "         616       0.00      0.00      0.00         1\n",
      "         618       0.00      0.00      0.00         0\n",
      "         619       0.00      0.00      0.00         0\n",
      "         623       0.00      0.00      0.00         1\n",
      "         624       0.00      0.00      0.00         1\n",
      "         625       0.00      0.00      0.00         5\n",
      "         626       0.00      0.00      0.00         3\n",
      "         627       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         0\n",
      "         635       0.00      0.00      0.00         0\n",
      "         636       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         0\n",
      "         641       0.00      0.00      0.00         1\n",
      "         648       0.00      0.00      0.00         0\n",
      "         652       0.00      0.00      0.00         1\n",
      "         655       0.00      0.00      0.00         0\n",
      "         660       0.00      0.00      0.00         3\n",
      "         665       0.00      0.00      0.00         1\n",
      "         666       0.00      0.00      0.00         1\n",
      "         670       0.00      0.00      0.00         0\n",
      "         671       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         0\n",
      "         682       0.00      0.00      0.00         1\n",
      "         684       0.00      0.00      0.00         0\n",
      "         685       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         0\n",
      "         688       0.00      0.00      0.00         0\n",
      "         691       0.00      0.00      0.00         1\n",
      "         697       0.00      0.00      0.00         1\n",
      "         702       0.00      0.00      0.00         0\n",
      "         706       0.00      0.00      0.00         0\n",
      "         707       0.00      0.00      0.00         0\n",
      "         709       0.00      0.00      0.00         1\n",
      "         712       0.00      0.00      0.00         0\n",
      "         713       0.00      0.00      0.00         1\n",
      "         714       0.00      0.00      0.00         1\n",
      "         716       0.00      0.00      0.00         0\n",
      "         719       0.00      0.00      0.00         4\n",
      "         720       0.00      0.00      0.00         0\n",
      "         721       0.00      0.00      0.00         1\n",
      "         722       0.00      0.00      0.00         1\n",
      "         723       0.00      0.00      0.00         0\n",
      "         724       0.00      0.00      0.00         0\n",
      "         725       0.00      0.00      0.00         0\n",
      "         726       0.00      0.00      0.00         0\n",
      "         728       0.00      0.00      0.00         0\n",
      "         730       0.00      0.00      0.00         1\n",
      "         731       0.00      0.00      0.00         0\n",
      "         732       0.00      0.00      0.00         1\n",
      "         733       0.00      0.00      0.00         1\n",
      "         736       0.00      0.00      0.00         1\n",
      "         737       0.00      0.00      0.00         1\n",
      "         739       0.00      0.00      0.00         1\n",
      "         740       0.00      0.00      0.00         0\n",
      "         742       0.00      0.00      0.00         1\n",
      "         745       0.00      0.00      0.00         0\n",
      "         747       0.00      0.00      0.00         1\n",
      "         748       0.00      0.00      0.00         0\n",
      "         750       0.00      0.00      0.00         0\n",
      "         753       0.00      0.00      0.00         1\n",
      "         758       0.00      0.00      0.00         9\n",
      "         760       0.00      0.00      0.00         3\n",
      "         761       0.00      0.00      0.00         1\n",
      "         763       0.00      0.00      0.00         0\n",
      "         764       0.00      0.00      0.00         1\n",
      "         765       0.00      0.00      0.00         6\n",
      "         767       0.00      0.00      0.00         1\n",
      "         768       0.00      0.00      0.00         0\n",
      "         770       0.00      0.00      0.00         2\n",
      "         772       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         6\n",
      "         775       0.00      0.00      0.00         1\n",
      "         778       0.00      0.00      0.00         1\n",
      "         780       0.00      0.00      0.00         0\n",
      "         783       0.00      0.00      0.00         0\n",
      "         787       0.00      0.00      0.00         0\n",
      "         788       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         1\n",
      "         791       0.00      0.00      0.00         2\n",
      "         792       0.00      0.00      0.00         0\n",
      "         795       0.00      0.00      0.00         0\n",
      "         796       0.00      0.00      0.00         1\n",
      "         804       0.00      0.00      0.00         0\n",
      "         807       0.00      0.00      0.00         0\n",
      "         808       0.00      0.00      0.00         1\n",
      "         810       0.00      0.00      0.00         3\n",
      "         815       0.00      0.00      0.00         0\n",
      "         817       0.00      0.00      0.00         1\n",
      "         818       0.00      0.00      0.00         0\n",
      "         819       0.00      0.00      0.00         1\n",
      "         820       0.00      0.00      0.00         1\n",
      "         821       0.00      0.00      0.00         1\n",
      "         825       0.00      0.00      0.00         1\n",
      "         826       0.00      0.00      0.00         1\n",
      "         827       0.00      0.00      0.00         1\n",
      "         828       0.03      0.03      0.03        68\n",
      "         829       0.00      0.00      0.00         0\n",
      "         831       0.00      0.00      0.00         1\n",
      "         832       0.00      0.00      0.00         1\n",
      "         833       0.00      0.00      0.00         4\n",
      "         834       0.00      0.00      0.00         6\n",
      "         836       0.00      0.00      0.00         0\n",
      "         837       0.00      0.00      0.00        12\n",
      "         838       0.00      0.00      0.00         0\n",
      "         840       0.00      0.00      0.00         1\n",
      "         841       0.00      0.00      0.00        10\n",
      "         842       0.00      0.00      0.00         0\n",
      "         843       0.00      0.00      0.00         1\n",
      "         845       0.00      0.00      0.00         1\n",
      "         847       0.00      0.00      0.00         1\n",
      "         851       0.00      0.00      0.00         1\n",
      "         854       0.00      0.00      0.00         2\n",
      "         856       0.00      0.00      0.00         1\n",
      "         859       0.00      0.00      0.00         1\n",
      "         861       0.00      0.00      0.00         1\n",
      "         865       0.00      0.00      0.00         6\n",
      "         867       0.00      0.00      0.00         0\n",
      "         868       0.00      0.00      0.00         3\n",
      "         870       0.00      0.00      0.00         0\n",
      "         872       0.00      0.00      0.00         0\n",
      "         874       0.00      0.00      0.00         2\n",
      "         875       0.00      0.00      0.00         1\n",
      "         877       0.00      0.00      0.00         0\n",
      "         878       0.00      0.00      0.00         3\n",
      "         879       0.00      0.00      0.00         1\n",
      "         880       0.00      0.00      0.00         1\n",
      "         882       0.00      0.00      0.00         1\n",
      "         883       0.00      0.00      0.00         1\n",
      "         886       0.00      0.00      0.00         5\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         1\n",
      "         892       0.00      0.00      0.00         1\n",
      "         894       0.00      0.00      0.00         2\n",
      "         895       0.20      0.08      0.11        13\n",
      "         897       0.00      0.00      0.00         2\n",
      "         898       0.00      0.00      0.00         1\n",
      "         899       0.00      0.00      0.00         1\n",
      "         904       0.00      0.00      0.00         0\n",
      "         905       0.00      0.00      0.00         1\n",
      "         906       0.00      0.00      0.00         0\n",
      "         909       0.00      0.00      0.00         2\n",
      "         910       0.00      0.00      0.00         0\n",
      "         911       0.00      0.00      0.00         0\n",
      "         913       0.00      0.00      0.00         2\n",
      "         914       0.00      0.00      0.00         1\n",
      "         916       0.00      0.00      0.00         2\n",
      "         919       0.00      0.00      0.00         1\n",
      "         920       0.00      0.00      0.00         1\n",
      "         921       0.00      0.00      0.00         1\n",
      "         926       0.00      0.00      0.00         1\n",
      "         928       0.00      0.00      0.00         1\n",
      "         931       0.00      0.00      0.00         1\n",
      "         932       0.00      0.00      0.00         1\n",
      "         933       0.00      0.00      0.00         1\n",
      "         934       0.00      0.00      0.00         1\n",
      "         940       0.00      0.00      0.00         1\n",
      "         941       0.00      0.00      0.00         1\n",
      "         945       0.00      0.00      0.00         1\n",
      "         946       0.00      0.00      0.00         0\n",
      "         947       0.00      0.00      0.00         1\n",
      "         951       0.00      0.00      0.00         4\n",
      "         953       0.00      0.00      0.00         1\n",
      "         955       0.00      0.00      0.00         1\n",
      "         958       0.00      0.00      0.00         0\n",
      "         960       0.00      0.00      0.00         1\n",
      "         963       0.00      0.00      0.00         0\n",
      "         964       0.00      0.00      0.00         1\n",
      "         966       0.00      0.00      0.00         0\n",
      "         967       0.00      0.00      0.00         1\n",
      "         972       0.00      0.00      0.00         1\n",
      "         975       0.00      0.00      0.00         1\n",
      "         978       0.00      0.00      0.00         1\n",
      "         980       0.00      0.00      0.00         1\n",
      "         982       0.11      0.07      0.09        14\n",
      "         984       0.00      0.00      0.00         1\n",
      "         985       0.00      0.00      0.00         1\n",
      "         986       0.04      0.07      0.05        14\n",
      "         987       0.00      0.00      0.00         0\n",
      "         993       0.00      0.00      0.00         1\n",
      "         995       0.00      0.00      0.00         4\n",
      "        1000       0.00      0.00      0.00         0\n",
      "        1005       0.00      0.00      0.00         1\n",
      "        1008       0.00      0.00      0.00         0\n",
      "        1009       0.00      0.00      0.00         0\n",
      "        1010       0.00      0.00      0.00         1\n",
      "        1012       0.00      0.00      0.00         1\n",
      "        1017       0.00      0.00      0.00         0\n",
      "        1018       0.00      0.00      0.00         1\n",
      "        1019       0.00      0.00      0.00         0\n",
      "        1020       0.00      0.00      0.00         1\n",
      "        1022       0.00      0.00      0.00         2\n",
      "        1024       0.00      0.00      0.00         0\n",
      "        1025       0.00      0.00      0.00         0\n",
      "        1030       0.00      0.00      0.00         1\n",
      "        1031       0.10      0.05      0.07        19\n",
      "        1034       0.00      0.00      0.00         0\n",
      "        1039       0.00      0.00      0.00         1\n",
      "        1040       0.00      0.00      0.00         1\n",
      "        1043       0.00      0.00      0.00         0\n",
      "        1044       0.00      0.00      0.00         1\n",
      "        1048       0.00      0.00      0.00         0\n",
      "        1050       0.50      0.50      0.50         2\n",
      "        1051       0.00      0.00      0.00         1\n",
      "        1055       0.00      0.00      0.00         0\n",
      "        1057       0.00      0.00      0.00         1\n",
      "        1058       0.00      0.00      0.00         0\n",
      "        1059       0.00      0.00      0.00         1\n",
      "        1061       0.00      0.00      0.00         1\n",
      "        1063       0.00      0.00      0.00         1\n",
      "        1064       0.00      0.00      0.00         1\n",
      "        1065       0.00      0.00      0.00         5\n",
      "        1068       0.00      0.00      0.00         1\n",
      "        1070       0.00      0.00      0.00         0\n",
      "        1071       0.00      0.00      0.00         1\n",
      "        1072       0.00      0.00      0.00         1\n",
      "        1075       0.00      0.00      0.00         1\n",
      "        1077       0.00      0.00      0.00         1\n",
      "        1078       0.00      0.00      0.00         1\n",
      "        1079       0.00      0.00      0.00         2\n",
      "        1080       0.00      0.00      0.00         0\n",
      "        1081       0.00      0.00      0.00         1\n",
      "        1083       0.00      0.00      0.00         0\n",
      "        1084       0.00      0.00      0.00         1\n",
      "        1091       0.00      0.00      0.00         0\n",
      "        1094       0.00      0.00      0.00         1\n",
      "        1095       0.00      0.00      0.00         0\n",
      "        1098       0.00      0.00      0.00         2\n",
      "        1099       0.00      0.00      0.00         0\n",
      "        1100       0.00      0.00      0.00         0\n",
      "        1101       0.00      0.00      0.00         0\n",
      "        1102       0.00      0.00      0.00         8\n",
      "        1104       0.00      0.00      0.00         0\n",
      "        1108       0.00      0.00      0.00         0\n",
      "        1111       0.00      0.00      0.00         5\n",
      "        1112       0.00      0.00      0.00         0\n",
      "        1115       0.00      0.00      0.00         1\n",
      "        1116       0.00      0.00      0.00         2\n",
      "        1119       0.00      0.00      0.00         1\n",
      "        1120       0.00      0.00      0.00         0\n",
      "        1122       0.00      0.00      0.00         2\n",
      "        1123       0.00      0.00      0.00         0\n",
      "        1125       0.00      0.00      0.00         0\n",
      "        1126       0.00      0.00      0.00         5\n",
      "        1128       0.00      0.00      0.00         1\n",
      "        1130       0.00      0.00      0.00         0\n",
      "        1131       0.00      0.00      0.00         0\n",
      "        1132       0.00      0.00      0.00         1\n",
      "        1134       0.00      0.00      0.00         0\n",
      "        1135       0.00      0.00      0.00         2\n",
      "        1138       0.00      0.00      0.00         1\n",
      "        1140       0.00      0.00      0.00         1\n",
      "        1141       0.00      0.00      0.00         1\n",
      "        1142       0.00      0.00      0.00         0\n",
      "        1144       0.00      0.00      0.00         1\n",
      "        1147       0.00      0.00      0.00         1\n",
      "        1149       0.00      0.00      0.00         0\n",
      "        1151       0.00      0.00      0.00         0\n",
      "        1153       0.00      0.00      0.00         1\n",
      "        1157       0.00      0.00      0.00         1\n",
      "        1159       0.00      0.00      0.00         1\n",
      "        1161       0.00      0.00      0.00         1\n",
      "        1165       0.00      0.00      0.00         1\n",
      "        1170       0.00      0.00      0.00         3\n",
      "        1172       0.00      0.00      0.00         0\n",
      "        1173       0.00      0.00      0.00         0\n",
      "        1174       0.00      0.00      0.00         0\n",
      "        1176       0.00      0.00      0.00         0\n",
      "        1178       0.00      0.00      0.00         2\n",
      "        1180       0.00      0.00      0.00         1\n",
      "        1182       0.00      0.00      0.00         1\n",
      "        1184       0.00      0.00      0.00         1\n",
      "        1185       0.00      0.00      0.00         0\n",
      "        1186       0.00      0.00      0.00         1\n",
      "        1187       0.00      0.00      0.00         1\n",
      "        1190       0.00      0.00      0.00         1\n",
      "        1192       0.00      0.00      0.00         1\n",
      "        1196       0.00      0.00      0.00         1\n",
      "        1197       0.00      0.00      0.00         0\n",
      "        1198       0.00      0.00      0.00         1\n",
      "        1202       0.00      0.00      0.00         0\n",
      "        1204       0.00      0.00      0.00         1\n",
      "        1205       0.00      0.00      0.00         0\n",
      "        1207       0.00      0.00      0.00         1\n",
      "        1208       0.00      0.00      0.00         1\n",
      "        1212       0.00      0.00      0.00         6\n",
      "        1213       0.00      0.00      0.00         1\n",
      "        1215       0.00      0.00      0.00         1\n",
      "        1217       0.00      0.00      0.00         3\n",
      "        1221       0.00      0.00      0.00         0\n",
      "        1224       0.00      0.00      0.00         1\n",
      "        1225       0.00      0.00      0.00         1\n",
      "        1228       0.00      0.00      0.00         1\n",
      "        1229       0.00      0.00      0.00         0\n",
      "        1230       0.00      0.00      0.00         0\n",
      "        1231       0.00      0.00      0.00         1\n",
      "        1234       0.00      0.00      0.00         1\n",
      "        1235       0.00      0.00      0.00        13\n",
      "        1237       0.00      0.00      0.00         1\n",
      "        1243       0.00      0.00      0.00         1\n",
      "        1248       0.00      0.00      0.00         0\n",
      "        1250       0.00      0.00      0.00         1\n",
      "        1254       0.00      0.00      0.00         1\n",
      "        1259       0.00      0.00      0.00         1\n",
      "        1260       0.00      0.00      0.00         0\n",
      "        1261       0.00      0.00      0.00         1\n",
      "        1262       0.00      0.00      0.00        18\n",
      "        1264       0.00      0.00      0.00         2\n",
      "        1265       0.00      0.00      0.00         1\n",
      "        1272       0.00      0.00      0.00         2\n",
      "        1275       0.00      0.00      0.00        22\n",
      "        1277       0.00      0.00      0.00         4\n",
      "        1278       0.00      0.00      0.00         1\n",
      "        1283       0.00      0.00      0.00         0\n",
      "        1284       0.00      0.00      0.00         1\n",
      "        1287       0.00      0.00      0.00         1\n",
      "        1288       0.00      0.00      0.00        14\n",
      "        1290       0.00      0.00      0.00         3\n",
      "        1292       0.00      0.00      0.00         1\n",
      "        1294       0.00      0.00      0.00         0\n",
      "        1296       0.00      0.00      0.00         1\n",
      "        1298       0.00      0.00      0.00         0\n",
      "        1300       0.00      0.00      0.00         1\n",
      "        1304       0.00      0.00      0.00         1\n",
      "        1306       0.23      0.59      0.33       202\n",
      "        1310       0.00      0.00      0.00         0\n",
      "        1311       0.00      0.00      0.00         1\n",
      "        1314       0.00      0.00      0.00         0\n",
      "        1315       0.00      0.00      0.00         0\n",
      "        1316       0.00      0.00      0.00         0\n",
      "        1317       0.00      0.00      0.00         0\n",
      "        1320       0.00      0.00      0.00         1\n",
      "        1321       0.00      0.00      0.00         4\n",
      "        1323       0.00      0.00      0.00         2\n",
      "        1329       0.06      0.05      0.05       102\n",
      "        1333       0.00      0.00      0.00         2\n",
      "        1334       0.00      0.00      0.00        12\n",
      "        1336       0.00      0.00      0.00         1\n",
      "        1338       0.00      0.00      0.00         1\n",
      "        1340       0.00      0.00      0.00         1\n",
      "        1341       0.00      0.00      0.00         1\n",
      "        1343       0.00      0.00      0.00         1\n",
      "        1345       0.00      0.00      0.00         1\n",
      "        1346       0.00      0.00      0.00         0\n",
      "        1348       0.08      0.07      0.07        15\n",
      "        1352       0.00      0.00      0.00         1\n",
      "        1354       0.00      0.00      0.00         1\n",
      "        1355       0.00      0.00      0.00         3\n",
      "        1357       0.00      0.00      0.00         0\n",
      "        1358       0.00      0.00      0.00         0\n",
      "        1360       0.00      0.00      0.00         0\n",
      "        1361       0.00      0.00      0.00         1\n",
      "        1362       0.00      0.00      0.00         1\n",
      "        1367       0.00      0.00      0.00         0\n",
      "        1373       0.10      0.06      0.07        17\n",
      "        1375       0.00      0.00      0.00         1\n",
      "        1376       0.00      0.00      0.00         1\n",
      "        1378       0.00      0.00      0.00         1\n",
      "        1381       0.00      0.00      0.00        11\n",
      "        1383       0.00      0.00      0.00         1\n",
      "        1384       0.00      0.00      0.00         3\n",
      "        1386       0.00      0.00      0.00         2\n",
      "        1391       0.00      0.00      0.00         0\n",
      "        1392       0.00      0.00      0.00         1\n",
      "        1393       0.00      0.00      0.00         0\n",
      "        1394       0.00      0.00      0.00         0\n",
      "        1401       0.00      0.00      0.00         5\n",
      "        1402       0.00      0.00      0.00         0\n",
      "        1403       0.00      0.00      0.00         1\n",
      "        1404       0.00      0.00      0.00         1\n",
      "        1405       0.00      0.00      0.00         0\n",
      "        1406       0.00      0.00      0.00         3\n",
      "        1408       0.00      0.00      0.00         0\n",
      "        1409       0.00      0.00      0.00         1\n",
      "        1410       0.00      0.00      0.00         0\n",
      "        1411       0.00      0.00      0.00         0\n",
      "        1414       0.00      0.00      0.00         1\n",
      "        1417       0.00      0.00      0.00         0\n",
      "        1419       0.00      0.00      0.00         1\n",
      "        1421       0.00      0.00      0.00         0\n",
      "        1422       0.00      0.00      0.00         3\n",
      "        1427       0.00      0.00      0.00         1\n",
      "        1428       0.00      0.00      0.00         1\n",
      "        1430       0.00      0.00      0.00         1\n",
      "        1433       0.00      0.00      0.00         0\n",
      "        1435       0.00      0.00      0.00         1\n",
      "        1437       0.00      0.00      0.00         0\n",
      "        1439       0.00      0.00      0.00         1\n",
      "        1441       0.00      0.00      0.00         1\n",
      "        1443       0.00      0.00      0.00         1\n",
      "        1444       0.00      0.00      0.00        14\n",
      "        1446       0.00      0.00      0.00         1\n",
      "        1448       0.00      0.00      0.00         4\n",
      "        1458       0.00      0.00      0.00         1\n",
      "        1462       0.00      0.00      0.00         0\n",
      "        1464       0.00      0.00      0.00         1\n",
      "        1465       0.00      0.00      0.00         1\n",
      "        1466       0.00      0.00      0.00         0\n",
      "        1468       0.00      0.00      0.00         1\n",
      "        1469       0.00      0.00      0.00         1\n",
      "        1470       0.00      0.00      0.00         0\n",
      "        1472       0.00      0.00      0.00         1\n",
      "        1473       0.00      0.00      0.00         0\n",
      "        1474       0.00      0.00      0.00         0\n",
      "        1475       0.00      0.00      0.00         1\n",
      "        1476       0.00      0.00      0.00         1\n",
      "        1477       0.00      0.00      0.00         1\n",
      "        1478       0.00      0.00      0.00         1\n",
      "        1479       0.00      0.00      0.00         1\n",
      "        1480       0.00      0.00      0.00         0\n",
      "        1484       0.00      0.00      0.00         1\n",
      "        1488       0.00      0.00      0.00         0\n",
      "        1489       0.00      0.00      0.00         0\n",
      "        1491       0.00      0.00      0.00         1\n",
      "        1492       0.00      0.00      0.00         0\n",
      "        1496       0.00      0.00      0.00         2\n",
      "        1497       0.00      0.00      0.00         0\n",
      "        1503       0.00      0.00      0.00         1\n",
      "        1506       0.00      0.00      0.00         1\n",
      "        1508       0.00      0.00      0.00         2\n",
      "        1510       0.00      0.00      0.00         1\n",
      "        1512       0.00      0.00      0.00         1\n",
      "        1514       0.07      0.11      0.09        46\n",
      "        1516       0.00      0.00      0.00         1\n",
      "        1518       0.00      0.00      0.00         2\n",
      "        1519       0.00      0.00      0.00         1\n",
      "        1520       0.08      0.11      0.09        37\n",
      "        1521       0.00      0.00      0.00         1\n",
      "        1522       0.00      0.00      0.00         0\n",
      "        1524       0.00      0.00      0.00         1\n",
      "        1525       0.00      0.00      0.00         1\n",
      "        1527       0.00      0.00      0.00         0\n",
      "        1528       0.00      0.00      0.00         1\n",
      "        1529       0.00      0.00      0.00         0\n",
      "        1530       0.00      0.00      0.00         1\n",
      "        1533       0.00      0.00      0.00         1\n",
      "        1542       0.00      0.00      0.00         0\n",
      "        1543       0.00      0.00      0.00         0\n",
      "        1547       0.00      0.00      0.00         1\n",
      "        1550       0.00      0.00      0.00         1\n",
      "        1551       0.00      0.00      0.00         1\n",
      "        1553       0.00      0.00      0.00         1\n",
      "        1554       0.00      0.00      0.00         7\n",
      "        1557       0.00      0.00      0.00         1\n",
      "        1558       0.00      0.00      0.00         0\n",
      "        1559       0.00      0.00      0.00         9\n",
      "        1560       0.00      0.00      0.00         7\n",
      "        1562       0.00      0.00      0.00         0\n",
      "        1565       0.00      0.00      0.00         0\n",
      "        1568       0.00      0.00      0.00         1\n",
      "        1569       0.00      0.00      0.00         1\n",
      "        1570       0.00      0.00      0.00         2\n",
      "        1571       0.00      0.00      0.00         2\n",
      "        1573       0.00      0.00      0.00         3\n",
      "        1574       0.00      0.00      0.00         1\n",
      "        1578       0.00      0.00      0.00         1\n",
      "        1579       0.00      0.00      0.00         1\n",
      "        1584       0.00      0.00      0.00         0\n",
      "        1585       0.00      0.00      0.00         1\n",
      "        1587       0.00      0.00      0.00         1\n",
      "        1588       0.00      0.00      0.00         1\n",
      "        1590       0.00      0.00      0.00         5\n",
      "        1592       0.00      0.00      0.00         1\n",
      "        1594       0.00      0.00      0.00         1\n",
      "        1601       0.00      0.00      0.00         0\n",
      "        1604       0.00      0.00      0.00         1\n",
      "        1605       0.00      0.00      0.00         0\n",
      "        1606       0.00      0.00      0.00         1\n",
      "        1608       0.00      0.00      0.00         1\n",
      "        1609       0.00      0.00      0.00         0\n",
      "        1610       0.00      0.00      0.00         1\n",
      "        1617       0.00      0.00      0.00         1\n",
      "        1618       0.00      0.00      0.00         9\n",
      "        1619       0.00      0.00      0.00         1\n",
      "        1621       0.00      0.00      0.00         1\n",
      "        1622       1.00      1.00      1.00         1\n",
      "        1626       0.00      0.00      0.00        29\n",
      "        1627       0.00      0.00      0.00         1\n",
      "        1629       0.00      0.00      0.00         0\n",
      "        1631       0.00      0.00      0.00         4\n",
      "        1633       0.00      0.00      0.00         1\n",
      "        1636       0.00      0.00      0.00         1\n",
      "        1637       0.00      0.00      0.00         1\n",
      "        1639       0.00      0.00      0.00         0\n",
      "        1640       0.00      0.00      0.00         1\n",
      "        1645       0.00      0.00      0.00         2\n",
      "        1646       0.00      0.00      0.00         0\n",
      "        1650       0.00      0.00      0.00         8\n",
      "        1651       0.00      0.00      0.00         2\n",
      "        1652       0.00      0.00      0.00         1\n",
      "        1653       0.00      0.00      0.00         1\n",
      "        1655       0.00      0.00      0.00         8\n",
      "        1656       0.00      0.00      0.00         1\n",
      "        1657       0.00      0.00      0.00         1\n",
      "        1659       0.00      0.00      0.00         0\n",
      "        1663       0.00      0.00      0.00         1\n",
      "        1667       0.00      0.00      0.00         1\n",
      "        1678       0.00      0.00      0.00         0\n",
      "        1680       0.00      0.00      0.00         1\n",
      "        1682       0.00      0.00      0.00         1\n",
      "        1683       0.00      0.00      0.00         1\n",
      "        1693       0.00      0.00      0.00         1\n",
      "        1694       0.00      0.00      0.00         1\n",
      "        1695       0.00      0.00      0.00         1\n",
      "        1698       0.00      0.00      0.00         1\n",
      "        1699       0.00      0.00      0.00        22\n",
      "        1701       0.00      0.00      0.00         1\n",
      "        1704       0.00      0.00      0.00         0\n",
      "        1705       0.00      0.00      0.00         6\n",
      "        1708       0.00      0.00      0.00         1\n",
      "        1709       0.00      0.00      0.00         1\n",
      "        1710       0.00      0.00      0.00         1\n",
      "        1712       0.00      0.00      0.00         1\n",
      "        1716       0.00      0.00      0.00         1\n",
      "        1718       0.00      0.00      0.00         1\n",
      "        1720       0.00      0.00      0.00         1\n",
      "        1722       0.00      0.00      0.00         5\n",
      "        1723       0.00      0.00      0.00        10\n",
      "        1724       0.00      0.00      0.00         1\n",
      "        1725       0.00      0.00      0.00         2\n",
      "        1727       0.00      0.00      0.00         1\n",
      "        1730       0.00      0.00      0.00         1\n",
      "        1731       0.00      0.00      0.00         1\n",
      "        1732       0.00      0.00      0.00         1\n",
      "        1736       0.00      0.00      0.00         1\n",
      "        1737       0.00      0.00      0.00         0\n",
      "        1740       0.00      0.00      0.00         0\n",
      "        1741       0.00      0.00      0.00         1\n",
      "        1743       0.00      0.00      0.00         1\n",
      "        1744       0.00      0.00      0.00         1\n",
      "        1746       0.00      0.00      0.00         1\n",
      "        1749       0.08      0.03      0.04        33\n",
      "        1750       0.00      0.00      0.00         2\n",
      "        1751       0.00      0.00      0.00         2\n",
      "        1754       0.00      0.00      0.00         1\n",
      "        1756       0.00      0.00      0.00         0\n",
      "        1757       0.00      0.00      0.00         1\n",
      "        1758       0.00      0.00      0.00         1\n",
      "        1760       0.00      0.00      0.00         2\n",
      "        1761       0.00      0.00      0.00         2\n",
      "        1764       0.00      0.00      0.00         0\n",
      "        1765       0.00      0.00      0.00         4\n",
      "        1766       0.00      0.00      0.00         1\n",
      "        1767       0.00      0.00      0.00         0\n",
      "        1768       0.00      0.00      0.00         0\n",
      "        1769       0.00      0.00      0.00         2\n",
      "        1771       0.00      0.00      0.00         2\n",
      "        1775       0.00      0.00      0.00         2\n",
      "        1778       0.00      0.00      0.00         2\n",
      "        1779       0.00      0.00      0.00         1\n",
      "        1780       0.00      0.00      0.00         1\n",
      "        1781       0.00      0.00      0.00         0\n",
      "        1787       0.00      0.00      0.00         1\n",
      "        1792       0.00      0.00      0.00         2\n",
      "        1793       0.00      0.00      0.00         1\n",
      "        1794       0.00      0.00      0.00         0\n",
      "        1795       0.00      0.00      0.00         0\n",
      "        1799       0.00      0.00      0.00         1\n",
      "        1807       0.00      0.00      0.00         3\n",
      "        1809       0.00      0.00      0.00         1\n",
      "        1810       0.00      0.00      0.00         0\n",
      "        1813       0.00      0.00      0.00         0\n",
      "        1815       0.00      0.00      0.00         1\n",
      "        1818       0.00      0.00      0.00         2\n",
      "        1819       0.00      0.00      0.00         0\n",
      "        1824       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.08      1911\n",
      "   macro avg       0.01      0.01      0.01      1911\n",
      "weighted avg       0.04      0.08      0.05      1911\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "430 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "22 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9592832 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4882432 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4792320 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4796416 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9764864 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2398208 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9584640 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2396160 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1199104 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2441216 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "17 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19185664 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 39059456 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 38338560 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 38371328 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19529728 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19169280 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1220608 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 76742656 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 78118912 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 76677120 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4796416 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4882432 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "17 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9584640 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19185664 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9764864 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19169280 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 4792320 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 9592832 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 599552 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2398208 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2441216 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1199104 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2396160 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 76742656 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 78118912 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 76677120 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 39059456 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 38338560 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 38371328 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "22 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 190, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 959, in fit\n",
      "    super()._fit(\n",
      "  File \"c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 165, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 266, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 787, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 758, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 37, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 19529728 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.10523535        nan        nan\n",
      "        nan        nan        nan 0.1071986         nan        nan\n",
      " 0.10876938        nan        nan        nan        nan        nan\n",
      " 0.10876928        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.10078474        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.10157008        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;bootstrap&#x27;: [True, False], &#x27;max_depth&#x27;: [10, 20, 30],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;bootstrap&#x27;: [True, False], &#x27;max_depth&#x27;: [10, 20, 30],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True, False], 'max_depth': [10, 20, 30],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HP\\healthcare\\noman\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "report = classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Accuracy: 0.11721611721611722\n",
      "Precision: 0.050842318690310084\n",
      "Recall: 0.11721611721611722\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           6       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         3\n",
      "          55       0.00      0.00      0.00         3\n",
      "          58       0.00      0.00      0.00         8\n",
      "          59       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         1\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00         0\n",
      "         101       0.00      0.00      0.00         1\n",
      "         102       0.00      0.00      0.00         2\n",
      "         104       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         1\n",
      "         109       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         5\n",
      "         120       0.00      0.00      0.00         1\n",
      "         121       0.00      0.00      0.00         2\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.00      0.00      0.00         1\n",
      "         136       0.00      0.00      0.00         1\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         0\n",
      "         168       0.00      0.00      0.00         1\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         177       0.11      0.11      0.11        36\n",
      "         181       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         2\n",
      "         186       0.10      0.06      0.08        32\n",
      "         191       0.00      0.00      0.00         9\n",
      "         192       0.00      0.00      0.00         0\n",
      "         196       0.00      0.00      0.00         1\n",
      "         200       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00        21\n",
      "         202       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         1\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         3\n",
      "         218       0.00      0.00      0.00         2\n",
      "         223       0.00      0.00      0.00         1\n",
      "         226       0.00      0.00      0.00         1\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.17      0.08      0.11        12\n",
      "         233       0.00      0.00      0.00         3\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         1\n",
      "         237       0.00      0.00      0.00         1\n",
      "         239       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         8\n",
      "         246       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         3\n",
      "         251       0.00      0.00      0.00         2\n",
      "         252       0.00      0.00      0.00         1\n",
      "         254       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         0\n",
      "         258       1.00      1.00      1.00         1\n",
      "         259       0.00      0.00      0.00         1\n",
      "         260       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         0\n",
      "         269       0.00      0.00      0.00         1\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         1\n",
      "         282       0.00      0.00      0.00         1\n",
      "         285       0.00      0.00      0.00         0\n",
      "         286       0.00      0.00      0.00         1\n",
      "         288       0.00      0.00      0.00         1\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         297       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         307       0.00      0.00      0.00         1\n",
      "         308       0.00      0.00      0.00         1\n",
      "         310       0.00      0.00      0.00        10\n",
      "         311       0.00      0.00      0.00         1\n",
      "         314       0.00      0.00      0.00         1\n",
      "         320       0.00      0.00      0.00         1\n",
      "         322       0.00      0.00      0.00         1\n",
      "         324       0.00      0.00      0.00         0\n",
      "         327       0.00      0.00      0.00         0\n",
      "         329       0.00      0.00      0.00         2\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.09      0.20      0.12        70\n",
      "         344       0.00      0.00      0.00         3\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       0.00      0.00      0.00         3\n",
      "         348       0.00      0.00      0.00         1\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         1\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         2\n",
      "         364       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.00      0.00      0.00         1\n",
      "         373       0.00      0.00      0.00         1\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         1\n",
      "         379       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         1\n",
      "         384       0.00      0.00      0.00         1\n",
      "         388       0.00      0.00      0.00         1\n",
      "         395       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         1\n",
      "         398       0.00      0.00      0.00         1\n",
      "         399       0.00      0.00      0.00         1\n",
      "         400       0.00      0.00      0.00         1\n",
      "         407       0.00      0.00      0.00         1\n",
      "         410       0.00      0.00      0.00         1\n",
      "         412       0.00      0.00      0.00         2\n",
      "         416       0.00      0.00      0.00         1\n",
      "         419       0.00      0.00      0.00         1\n",
      "         420       0.00      0.00      0.00         1\n",
      "         422       0.00      0.00      0.00         1\n",
      "         426       0.00      0.00      0.00         1\n",
      "         433       0.00      0.00      0.00         2\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         1\n",
      "         447       0.00      0.00      0.00         1\n",
      "         451       0.00      0.00      0.00         1\n",
      "         455       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         470       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         3\n",
      "         493       0.00      0.00      0.00         0\n",
      "         497       0.01      0.02      0.01        61\n",
      "         498       0.00      0.00      0.00         1\n",
      "         500       0.00      0.00      0.00         1\n",
      "         502       0.00      0.00      0.00         1\n",
      "         504       0.00      0.00      0.00         1\n",
      "         505       0.00      0.00      0.00         1\n",
      "         509       0.00      0.00      0.00         1\n",
      "         512       0.00      0.00      0.00         2\n",
      "         517       0.00      0.00      0.00         0\n",
      "         518       0.00      0.00      0.00        19\n",
      "         521       0.00      0.00      0.00         1\n",
      "         533       0.00      0.00      0.00         1\n",
      "         536       0.00      0.00      0.00         1\n",
      "         537       0.00      0.00      0.00         1\n",
      "         545       0.00      0.00      0.00         2\n",
      "         546       0.00      0.00      0.00         4\n",
      "         547       0.00      0.00      0.00         1\n",
      "         549       0.00      0.00      0.00        22\n",
      "         551       0.00      0.00      0.00         1\n",
      "         552       0.00      0.00      0.00         1\n",
      "         554       0.00      0.00      0.00         1\n",
      "         555       0.00      0.00      0.00         2\n",
      "         556       0.00      0.00      0.00         1\n",
      "         561       0.00      0.00      0.00         1\n",
      "         563       0.00      0.00      0.00         1\n",
      "         564       0.00      0.00      0.00         2\n",
      "         565       0.00      0.00      0.00         1\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       0.00      0.00      0.00         2\n",
      "         578       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         1\n",
      "         582       0.00      0.00      0.00         2\n",
      "         583       0.00      0.00      0.00         1\n",
      "         588       0.00      0.00      0.00        11\n",
      "         589       0.00      0.00      0.00         3\n",
      "         591       0.00      0.00      0.00         2\n",
      "         593       0.00      0.00      0.00         1\n",
      "         595       0.00      0.00      0.00         1\n",
      "         600       0.00      0.00      0.00         0\n",
      "         601       0.00      0.00      0.00         1\n",
      "         602       0.00      0.00      0.00         1\n",
      "         604       0.00      0.00      0.00         2\n",
      "         610       0.00      0.00      0.00         1\n",
      "         611       0.00      0.00      0.00         1\n",
      "         613       0.00      0.00      0.00         1\n",
      "         614       0.00      0.00      0.00         1\n",
      "         615       0.00      0.00      0.00         1\n",
      "         616       0.00      0.00      0.00         1\n",
      "         623       0.00      0.00      0.00         1\n",
      "         624       0.00      0.00      0.00         1\n",
      "         625       0.00      0.00      0.00         5\n",
      "         626       0.00      0.00      0.00         3\n",
      "         627       0.00      0.00      0.00         1\n",
      "         636       0.00      0.00      0.00         1\n",
      "         641       0.00      0.00      0.00         1\n",
      "         652       0.00      0.00      0.00         1\n",
      "         660       0.00      0.00      0.00         3\n",
      "         665       0.00      0.00      0.00         1\n",
      "         666       0.00      0.00      0.00         1\n",
      "         672       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         681       0.00      0.00      0.00         0\n",
      "         682       0.00      0.00      0.00         1\n",
      "         691       0.00      0.00      0.00         1\n",
      "         697       0.00      0.00      0.00         1\n",
      "         709       0.00      0.00      0.00         1\n",
      "         713       0.00      0.00      0.00         1\n",
      "         714       0.00      0.00      0.00         1\n",
      "         719       0.00      0.00      0.00         4\n",
      "         721       0.00      0.00      0.00         1\n",
      "         722       0.00      0.00      0.00         1\n",
      "         730       0.00      0.00      0.00         1\n",
      "         732       0.00      0.00      0.00         1\n",
      "         733       0.00      0.00      0.00         1\n",
      "         736       0.00      0.00      0.00         1\n",
      "         737       0.00      0.00      0.00         1\n",
      "         739       0.00      0.00      0.00         1\n",
      "         742       0.00      0.00      0.00         1\n",
      "         747       0.00      0.00      0.00         1\n",
      "         750       0.00      0.00      0.00         0\n",
      "         753       0.00      0.00      0.00         1\n",
      "         758       0.00      0.00      0.00         9\n",
      "         760       0.00      0.00      0.00         3\n",
      "         761       0.00      0.00      0.00         1\n",
      "         764       0.00      0.00      0.00         1\n",
      "         765       0.00      0.00      0.00         6\n",
      "         767       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         2\n",
      "         772       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         6\n",
      "         775       0.00      0.00      0.00         1\n",
      "         778       0.00      0.00      0.00         1\n",
      "         788       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         1\n",
      "         791       0.00      0.00      0.00         2\n",
      "         796       0.00      0.00      0.00         1\n",
      "         808       0.00      0.00      0.00         1\n",
      "         810       0.00      0.00      0.00         3\n",
      "         817       0.00      0.00      0.00         1\n",
      "         818       0.00      0.00      0.00         0\n",
      "         819       0.00      0.00      0.00         1\n",
      "         820       0.00      0.00      0.00         1\n",
      "         821       0.00      0.00      0.00         1\n",
      "         825       0.00      0.00      0.00         1\n",
      "         826       0.00      0.00      0.00         1\n",
      "         827       0.00      0.00      0.00         1\n",
      "         828       0.08      0.10      0.09        68\n",
      "         829       0.00      0.00      0.00         0\n",
      "         831       0.00      0.00      0.00         1\n",
      "         832       0.00      0.00      0.00         1\n",
      "         833       0.00      0.00      0.00         4\n",
      "         834       0.00      0.00      0.00         6\n",
      "         836       0.00      0.00      0.00         0\n",
      "         837       0.00      0.00      0.00        12\n",
      "         840       0.00      0.00      0.00         1\n",
      "         841       0.00      0.00      0.00        10\n",
      "         843       0.00      0.00      0.00         1\n",
      "         845       0.00      0.00      0.00         1\n",
      "         847       0.00      0.00      0.00         1\n",
      "         851       0.00      0.00      0.00         1\n",
      "         854       0.00      0.00      0.00         2\n",
      "         856       0.00      0.00      0.00         1\n",
      "         859       0.00      0.00      0.00         1\n",
      "         861       0.00      0.00      0.00         1\n",
      "         865       0.00      0.00      0.00         6\n",
      "         868       0.00      0.00      0.00         3\n",
      "         874       0.00      0.00      0.00         2\n",
      "         875       0.00      0.00      0.00         1\n",
      "         878       0.00      0.00      0.00         3\n",
      "         879       0.00      0.00      0.00         1\n",
      "         880       0.00      0.00      0.00         1\n",
      "         882       0.00      0.00      0.00         1\n",
      "         883       0.00      0.00      0.00         1\n",
      "         886       0.00      0.00      0.00         5\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         1\n",
      "         892       0.00      0.00      0.00         1\n",
      "         894       0.00      0.00      0.00         2\n",
      "         895       0.28      0.38      0.32        13\n",
      "         897       0.00      0.00      0.00         2\n",
      "         898       0.00      0.00      0.00         1\n",
      "         899       0.00      0.00      0.00         1\n",
      "         905       0.00      0.00      0.00         1\n",
      "         909       0.00      0.00      0.00         2\n",
      "         913       0.00      0.00      0.00         2\n",
      "         914       0.00      0.00      0.00         1\n",
      "         916       0.00      0.00      0.00         2\n",
      "         919       0.00      0.00      0.00         1\n",
      "         920       0.00      0.00      0.00         1\n",
      "         921       0.00      0.00      0.00         1\n",
      "         926       0.00      0.00      0.00         1\n",
      "         928       0.00      0.00      0.00         1\n",
      "         931       0.00      0.00      0.00         1\n",
      "         932       0.00      0.00      0.00         1\n",
      "         933       0.00      0.00      0.00         1\n",
      "         934       0.00      0.00      0.00         1\n",
      "         940       0.00      0.00      0.00         1\n",
      "         941       0.00      0.00      0.00         1\n",
      "         945       0.00      0.00      0.00         1\n",
      "         947       0.00      0.00      0.00         1\n",
      "         950       0.00      0.00      0.00         0\n",
      "         951       0.00      0.00      0.00         4\n",
      "         953       0.00      0.00      0.00         1\n",
      "         955       0.00      0.00      0.00         1\n",
      "         960       0.00      0.00      0.00         1\n",
      "         964       0.00      0.00      0.00         1\n",
      "         967       0.00      0.00      0.00         1\n",
      "         972       0.00      0.00      0.00         1\n",
      "         975       0.00      0.00      0.00         1\n",
      "         978       0.00      0.00      0.00         1\n",
      "         980       0.00      0.00      0.00         1\n",
      "         982       0.33      0.07      0.12        14\n",
      "         984       0.00      0.00      0.00         1\n",
      "         985       0.00      0.00      0.00         1\n",
      "         986       0.00      0.00      0.00        14\n",
      "         993       0.00      0.00      0.00         1\n",
      "         995       0.00      0.00      0.00         4\n",
      "        1005       0.00      0.00      0.00         1\n",
      "        1009       0.00      0.00      0.00         0\n",
      "        1010       0.00      0.00      0.00         1\n",
      "        1012       0.00      0.00      0.00         1\n",
      "        1018       0.00      0.00      0.00         1\n",
      "        1020       0.00      0.00      0.00         1\n",
      "        1022       0.00      0.00      0.00         2\n",
      "        1024       0.00      0.00      0.00         0\n",
      "        1030       0.00      0.00      0.00         1\n",
      "        1031       0.07      0.11      0.08        19\n",
      "        1039       0.00      0.00      0.00         1\n",
      "        1040       0.00      0.00      0.00         1\n",
      "        1044       0.00      0.00      0.00         1\n",
      "        1050       0.00      0.00      0.00         2\n",
      "        1051       0.00      0.00      0.00         1\n",
      "        1057       0.00      0.00      0.00         1\n",
      "        1059       0.00      0.00      0.00         1\n",
      "        1061       0.00      0.00      0.00         1\n",
      "        1063       0.00      0.00      0.00         1\n",
      "        1064       0.00      0.00      0.00         1\n",
      "        1065       0.00      0.00      0.00         5\n",
      "        1068       0.00      0.00      0.00         1\n",
      "        1071       0.00      0.00      0.00         1\n",
      "        1072       0.00      0.00      0.00         1\n",
      "        1075       0.00      0.00      0.00         1\n",
      "        1077       0.00      0.00      0.00         1\n",
      "        1078       0.00      0.00      0.00         1\n",
      "        1079       0.00      0.00      0.00         2\n",
      "        1081       0.00      0.00      0.00         1\n",
      "        1084       0.00      0.00      0.00         1\n",
      "        1094       0.00      0.00      0.00         1\n",
      "        1098       0.00      0.00      0.00         2\n",
      "        1102       0.00      0.00      0.00         8\n",
      "        1111       0.00      0.00      0.00         5\n",
      "        1115       0.00      0.00      0.00         1\n",
      "        1116       0.00      0.00      0.00         2\n",
      "        1119       0.00      0.00      0.00         1\n",
      "        1122       0.00      0.00      0.00         2\n",
      "        1125       0.00      0.00      0.00         0\n",
      "        1126       0.00      0.00      0.00         5\n",
      "        1128       0.00      0.00      0.00         1\n",
      "        1132       0.00      0.00      0.00         1\n",
      "        1135       0.00      0.00      0.00         2\n",
      "        1138       0.00      0.00      0.00         1\n",
      "        1140       0.00      0.00      0.00         1\n",
      "        1141       0.00      0.00      0.00         1\n",
      "        1144       0.00      0.00      0.00         1\n",
      "        1147       0.00      0.00      0.00         1\n",
      "        1151       0.00      0.00      0.00         0\n",
      "        1153       0.00      0.00      0.00         1\n",
      "        1157       0.00      0.00      0.00         1\n",
      "        1159       0.00      0.00      0.00         1\n",
      "        1161       0.00      0.00      0.00         1\n",
      "        1165       0.00      0.00      0.00         1\n",
      "        1168       0.00      0.00      0.00         0\n",
      "        1170       0.00      0.00      0.00         3\n",
      "        1172       0.00      0.00      0.00         0\n",
      "        1178       0.00      0.00      0.00         2\n",
      "        1180       0.00      0.00      0.00         1\n",
      "        1182       0.00      0.00      0.00         1\n",
      "        1184       0.00      0.00      0.00         1\n",
      "        1186       0.00      0.00      0.00         1\n",
      "        1187       0.00      0.00      0.00         1\n",
      "        1190       0.00      0.00      0.00         1\n",
      "        1192       0.00      0.00      0.00         1\n",
      "        1196       0.00      0.00      0.00         1\n",
      "        1198       0.00      0.00      0.00         1\n",
      "        1204       0.00      0.00      0.00         1\n",
      "        1207       0.00      0.00      0.00         1\n",
      "        1208       0.00      0.00      0.00         1\n",
      "        1212       0.00      0.00      0.00         6\n",
      "        1213       0.00      0.00      0.00         1\n",
      "        1215       0.00      0.00      0.00         1\n",
      "        1217       0.00      0.00      0.00         3\n",
      "        1224       0.00      0.00      0.00         1\n",
      "        1225       0.00      0.00      0.00         1\n",
      "        1228       0.00      0.00      0.00         1\n",
      "        1231       0.00      0.00      0.00         1\n",
      "        1234       0.00      0.00      0.00         1\n",
      "        1235       0.00      0.00      0.00        13\n",
      "        1237       0.00      0.00      0.00         1\n",
      "        1243       0.00      0.00      0.00         1\n",
      "        1250       0.00      0.00      0.00         1\n",
      "        1254       0.00      0.00      0.00         1\n",
      "        1259       0.00      0.00      0.00         1\n",
      "        1261       0.00      0.00      0.00         1\n",
      "        1262       0.00      0.00      0.00        18\n",
      "        1264       0.00      0.00      0.00         2\n",
      "        1265       0.00      0.00      0.00         1\n",
      "        1272       0.00      0.00      0.00         2\n",
      "        1275       0.00      0.00      0.00        22\n",
      "        1277       0.00      0.00      0.00         4\n",
      "        1278       0.00      0.00      0.00         1\n",
      "        1284       0.00      0.00      0.00         1\n",
      "        1287       0.00      0.00      0.00         1\n",
      "        1288       0.10      0.07      0.08        14\n",
      "        1290       0.00      0.00      0.00         3\n",
      "        1292       0.00      0.00      0.00         1\n",
      "        1296       0.00      0.00      0.00         1\n",
      "        1300       0.00      0.00      0.00         1\n",
      "        1304       0.00      0.00      0.00         1\n",
      "        1306       0.20      0.72      0.31       202\n",
      "        1311       0.00      0.00      0.00         1\n",
      "        1320       0.00      0.00      0.00         1\n",
      "        1321       0.00      0.00      0.00         4\n",
      "        1323       0.00      0.00      0.00         2\n",
      "        1329       0.11      0.26      0.16       102\n",
      "        1333       0.00      0.00      0.00         2\n",
      "        1334       0.00      0.00      0.00        12\n",
      "        1336       0.00      0.00      0.00         1\n",
      "        1338       0.00      0.00      0.00         1\n",
      "        1340       0.00      0.00      0.00         1\n",
      "        1341       0.00      0.00      0.00         1\n",
      "        1343       0.00      0.00      0.00         1\n",
      "        1345       0.00      0.00      0.00         1\n",
      "        1348       0.00      0.00      0.00        15\n",
      "        1352       0.00      0.00      0.00         1\n",
      "        1354       0.00      0.00      0.00         1\n",
      "        1355       0.00      0.00      0.00         3\n",
      "        1357       0.00      0.00      0.00         0\n",
      "        1361       0.00      0.00      0.00         1\n",
      "        1362       0.00      0.00      0.00         1\n",
      "        1373       0.00      0.00      0.00        17\n",
      "        1375       0.00      0.00      0.00         1\n",
      "        1376       0.00      0.00      0.00         1\n",
      "        1378       0.00      0.00      0.00         1\n",
      "        1381       0.00      0.00      0.00        11\n",
      "        1383       0.00      0.00      0.00         1\n",
      "        1384       0.00      0.00      0.00         3\n",
      "        1386       0.00      0.00      0.00         2\n",
      "        1392       0.00      0.00      0.00         1\n",
      "        1401       0.50      0.20      0.29         5\n",
      "        1403       0.00      0.00      0.00         1\n",
      "        1404       0.00      0.00      0.00         1\n",
      "        1406       0.00      0.00      0.00         3\n",
      "        1409       0.00      0.00      0.00         1\n",
      "        1414       0.00      0.00      0.00         1\n",
      "        1419       0.00      0.00      0.00         1\n",
      "        1421       0.00      0.00      0.00         0\n",
      "        1422       0.00      0.00      0.00         3\n",
      "        1427       0.00      0.00      0.00         1\n",
      "        1428       0.00      0.00      0.00         1\n",
      "        1430       0.00      0.00      0.00         1\n",
      "        1435       0.00      0.00      0.00         1\n",
      "        1439       0.00      0.00      0.00         1\n",
      "        1441       0.00      0.00      0.00         1\n",
      "        1443       0.00      0.00      0.00         1\n",
      "        1444       0.00      0.00      0.00        14\n",
      "        1446       0.00      0.00      0.00         1\n",
      "        1448       0.00      0.00      0.00         4\n",
      "        1458       0.00      0.00      0.00         1\n",
      "        1464       0.00      0.00      0.00         1\n",
      "        1465       0.00      0.00      0.00         1\n",
      "        1468       0.00      0.00      0.00         1\n",
      "        1469       0.00      0.00      0.00         1\n",
      "        1472       0.00      0.00      0.00         1\n",
      "        1475       0.00      0.00      0.00         1\n",
      "        1476       0.00      0.00      0.00         1\n",
      "        1477       0.00      0.00      0.00         1\n",
      "        1478       0.00      0.00      0.00         1\n",
      "        1479       0.00      0.00      0.00         1\n",
      "        1484       0.00      0.00      0.00         1\n",
      "        1491       0.00      0.00      0.00         1\n",
      "        1496       0.00      0.00      0.00         2\n",
      "        1503       0.00      0.00      0.00         1\n",
      "        1506       0.00      0.00      0.00         1\n",
      "        1508       0.00      0.00      0.00         2\n",
      "        1510       0.00      0.00      0.00         1\n",
      "        1512       0.00      0.00      0.00         1\n",
      "        1514       0.03      0.13      0.05        46\n",
      "        1516       0.00      0.00      0.00         1\n",
      "        1518       0.00      0.00      0.00         2\n",
      "        1519       0.00      0.00      0.00         1\n",
      "        1520       0.06      0.05      0.06        37\n",
      "        1521       0.00      0.00      0.00         1\n",
      "        1524       0.00      0.00      0.00         1\n",
      "        1525       0.00      0.00      0.00         1\n",
      "        1528       0.00      0.00      0.00         1\n",
      "        1529       0.00      0.00      0.00         0\n",
      "        1530       0.00      0.00      0.00         1\n",
      "        1533       0.00      0.00      0.00         1\n",
      "        1547       0.00      0.00      0.00         1\n",
      "        1550       0.00      0.00      0.00         1\n",
      "        1551       0.00      0.00      0.00         1\n",
      "        1553       0.00      0.00      0.00         1\n",
      "        1554       0.00      0.00      0.00         7\n",
      "        1557       0.00      0.00      0.00         1\n",
      "        1559       0.00      0.00      0.00         9\n",
      "        1560       0.00      0.00      0.00         7\n",
      "        1568       0.00      0.00      0.00         1\n",
      "        1569       0.00      0.00      0.00         1\n",
      "        1570       0.00      0.00      0.00         2\n",
      "        1571       0.00      0.00      0.00         2\n",
      "        1573       0.00      0.00      0.00         3\n",
      "        1574       0.00      0.00      0.00         1\n",
      "        1578       0.00      0.00      0.00         1\n",
      "        1579       0.00      0.00      0.00         1\n",
      "        1585       0.00      0.00      0.00         1\n",
      "        1587       0.00      0.00      0.00         1\n",
      "        1588       0.00      0.00      0.00         1\n",
      "        1590       0.00      0.00      0.00         5\n",
      "        1592       0.00      0.00      0.00         1\n",
      "        1594       0.00      0.00      0.00         1\n",
      "        1604       0.00      0.00      0.00         1\n",
      "        1606       0.00      0.00      0.00         1\n",
      "        1608       0.00      0.00      0.00         1\n",
      "        1610       0.00      0.00      0.00         1\n",
      "        1617       0.00      0.00      0.00         1\n",
      "        1618       0.00      0.00      0.00         9\n",
      "        1619       0.00      0.00      0.00         1\n",
      "        1621       0.00      0.00      0.00         1\n",
      "        1622       1.00      1.00      1.00         1\n",
      "        1626       0.18      0.10      0.13        29\n",
      "        1627       0.00      0.00      0.00         1\n",
      "        1631       0.00      0.00      0.00         4\n",
      "        1633       0.00      0.00      0.00         1\n",
      "        1636       0.00      0.00      0.00         1\n",
      "        1637       0.00      0.00      0.00         1\n",
      "        1639       0.00      0.00      0.00         0\n",
      "        1640       0.00      0.00      0.00         1\n",
      "        1645       0.00      0.00      0.00         2\n",
      "        1646       0.00      0.00      0.00         0\n",
      "        1650       0.00      0.00      0.00         8\n",
      "        1651       0.00      0.00      0.00         2\n",
      "        1652       0.00      0.00      0.00         1\n",
      "        1653       0.00      0.00      0.00         1\n",
      "        1655       0.00      0.00      0.00         8\n",
      "        1656       0.00      0.00      0.00         1\n",
      "        1657       0.00      0.00      0.00         1\n",
      "        1663       0.00      0.00      0.00         1\n",
      "        1667       0.00      0.00      0.00         1\n",
      "        1680       0.00      0.00      0.00         1\n",
      "        1682       0.00      0.00      0.00         1\n",
      "        1683       0.00      0.00      0.00         1\n",
      "        1693       0.00      0.00      0.00         1\n",
      "        1694       0.00      0.00      0.00         1\n",
      "        1695       0.00      0.00      0.00         1\n",
      "        1698       0.00      0.00      0.00         1\n",
      "        1699       0.00      0.00      0.00        22\n",
      "        1701       0.00      0.00      0.00         1\n",
      "        1705       0.00      0.00      0.00         6\n",
      "        1708       0.00      0.00      0.00         1\n",
      "        1709       0.00      0.00      0.00         1\n",
      "        1710       0.00      0.00      0.00         1\n",
      "        1712       0.00      0.00      0.00         1\n",
      "        1716       0.00      0.00      0.00         1\n",
      "        1718       0.00      0.00      0.00         1\n",
      "        1720       0.00      0.00      0.00         1\n",
      "        1722       0.00      0.00      0.00         5\n",
      "        1723       0.00      0.00      0.00        10\n",
      "        1724       0.00      0.00      0.00         1\n",
      "        1725       0.00      0.00      0.00         2\n",
      "        1727       0.00      0.00      0.00         1\n",
      "        1730       0.00      0.00      0.00         1\n",
      "        1731       0.00      0.00      0.00         1\n",
      "        1732       0.00      0.00      0.00         1\n",
      "        1736       0.00      0.00      0.00         1\n",
      "        1741       0.00      0.00      0.00         1\n",
      "        1743       0.00      0.00      0.00         1\n",
      "        1744       0.00      0.00      0.00         1\n",
      "        1746       0.00      0.00      0.00         1\n",
      "        1749       0.00      0.00      0.00        33\n",
      "        1750       0.00      0.00      0.00         2\n",
      "        1751       0.00      0.00      0.00         2\n",
      "        1754       0.00      0.00      0.00         1\n",
      "        1757       0.00      0.00      0.00         1\n",
      "        1758       0.00      0.00      0.00         1\n",
      "        1760       0.00      0.00      0.00         2\n",
      "        1761       0.00      0.00      0.00         2\n",
      "        1765       0.00      0.00      0.00         4\n",
      "        1766       0.00      0.00      0.00         1\n",
      "        1769       0.00      0.00      0.00         2\n",
      "        1771       0.00      0.00      0.00         2\n",
      "        1775       0.00      0.00      0.00         2\n",
      "        1778       0.00      0.00      0.00         2\n",
      "        1779       0.00      0.00      0.00         1\n",
      "        1780       0.00      0.00      0.00         1\n",
      "        1787       0.00      0.00      0.00         1\n",
      "        1792       0.00      0.00      0.00         2\n",
      "        1793       0.00      0.00      0.00         1\n",
      "        1795       0.00      0.00      0.00         0\n",
      "        1799       0.00      0.00      0.00         1\n",
      "        1807       0.00      0.00      0.00         3\n",
      "        1809       0.00      0.00      0.00         1\n",
      "        1815       0.00      0.00      0.00         1\n",
      "        1818       0.00      0.00      0.00         2\n",
      "        1824       0.00      0.00      0.00         1\n",
      "        1825       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.12      1911\n",
      "   macro avg       0.01      0.01      0.01      1911\n",
      "weighted avg       0.05      0.12      0.06      1911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.95287958115183"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_train, y_train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(best_model, 'restaurant_cuisine_model.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
